{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFJ0cTAJ1ILD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeoDYJ-zQsq_",
        "outputId": "21fc6013-3d90-4f24-c4dd-e68e51dee4ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1Z1arCKX_Od"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGwsM2e01oO4"
      },
      "outputs": [],
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpYnatTA1rDZ",
        "outputId": "c2ea8a4e-f626-4aca-8cc5-0a6ed84980ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128, 128, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 64, 64, 32)   320         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 32, 32, 64)   18496       ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 16, 16, 128)  73856       ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 8, 8, 256)    295168      ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 16384)        0           ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 724)          11862740    ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 32)           23200       ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 32)           23200       ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " sampling (Sampling)            (None, 32)           0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 12,296,980\n",
            "Trainable params: 12,296,980\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "latent_dim = 32\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(128, 128, 1))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2D(128, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2D(256, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(724, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N46V1Qew1ttP",
        "outputId": "ea5a787e-4797-474e-9ec5-26bd4ce3144b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 32)]              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 724)               23892     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16384)             11878400  \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 16, 16, 256)      590080    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 128)      295040    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 64, 64, 64)       73792     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 128, 128, 32)     18464     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_transpose_4 (Conv2DT  (None, 128, 128, 1)      289       \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,879,957\n",
            "Trainable params: 12,879,957\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(724, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Dense(16384, activation=\"relu\")(x)\n",
        "x = layers.Reshape((8, 8, 256))(x)\n",
        "x = layers.Conv2DTranspose(256, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(128, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up6UsxUG1y3O"
      },
      "outputs": [],
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jFoCagIQ2PA"
      },
      "outputs": [],
      "source": [
        "dir_path = \"/content/gdrive/MyDrive/guided-diffusion/datasets/IITD_LQ_128/Data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40Ot352RQ8sO"
      },
      "outputs": [],
      "source": [
        "shape = (128, 128, 1)\n",
        "images = []\n",
        "for i in os.listdir(dir_path):\n",
        "  if(i[-3:]=='png'):\n",
        "    image_path = os.path.join(dir_path, i)\n",
        "    image = cv.imread(image_path)\n",
        "    image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "    # image = image[:, :, np.newaxis]\n",
        "    # print(type(image), np.shape(image))\n",
        "    images.append(image)\n",
        "images=np.array(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7RTQu5ESq-d",
        "outputId": "01b96be7-74f1-405d-d951-c147bf82f392"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(964, 128, 128)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.shape(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6EUGKSkUPJU"
      },
      "outputs": [],
      "source": [
        "images = np.expand_dims(images, -1).astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMKLYnyU2GAP",
        "outputId": "ec876d48-c8f3-4257-de6b-2dbb720f4db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "8/8 [==============================] - 23s 378ms/step - loss: 11576.1982 - reconstruction_loss: 11539.7383 - kl_loss: 42.8087\n",
            "Epoch 2/300\n",
            "8/8 [==============================] - 1s 143ms/step - loss: 11335.6099 - reconstruction_loss: 11340.4951 - kl_loss: 0.0113\n",
            "Epoch 3/300\n",
            "8/8 [==============================] - 1s 143ms/step - loss: 11343.3001 - reconstruction_loss: 11341.0117 - kl_loss: 0.0048\n",
            "Epoch 4/300\n",
            "8/8 [==============================] - 1s 144ms/step - loss: 11321.5788 - reconstruction_loss: 11312.5098 - kl_loss: 0.0488\n",
            "Epoch 5/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 11199.3698 - reconstruction_loss: 11128.3076 - kl_loss: 0.9064\n",
            "Epoch 6/300\n",
            "8/8 [==============================] - 1s 158ms/step - loss: 10853.2120 - reconstruction_loss: 10823.1523 - kl_loss: 3.7909\n",
            "Epoch 7/300\n",
            "8/8 [==============================] - 1s 162ms/step - loss: 10724.8673 - reconstruction_loss: 10714.3799 - kl_loss: 0.3438\n",
            "Epoch 8/300\n",
            "8/8 [==============================] - 1s 157ms/step - loss: 10675.4300 - reconstruction_loss: 10658.1123 - kl_loss: 0.1880\n",
            "Epoch 9/300\n",
            "8/8 [==============================] - 1s 143ms/step - loss: 10672.7254 - reconstruction_loss: 10654.2354 - kl_loss: 0.1467\n",
            "Epoch 10/300\n",
            "8/8 [==============================] - 1s 143ms/step - loss: 10640.4045 - reconstruction_loss: 10633.1094 - kl_loss: 0.1155\n",
            "Epoch 11/300\n",
            "8/8 [==============================] - 1s 145ms/step - loss: 10619.6591 - reconstruction_loss: 10625.1240 - kl_loss: 0.0989\n",
            "Epoch 12/300\n",
            "8/8 [==============================] - 1s 144ms/step - loss: 10611.0326 - reconstruction_loss: 10612.2822 - kl_loss: 0.0911\n",
            "Epoch 13/300\n",
            "8/8 [==============================] - 1s 145ms/step - loss: 10602.4363 - reconstruction_loss: 10603.0547 - kl_loss: 0.0860\n",
            "Epoch 14/300\n",
            "8/8 [==============================] - 1s 145ms/step - loss: 10588.4868 - reconstruction_loss: 10596.5889 - kl_loss: 0.0817\n",
            "Epoch 15/300\n",
            "8/8 [==============================] - 1s 145ms/step - loss: 10590.0360 - reconstruction_loss: 10583.1064 - kl_loss: 0.0765\n",
            "Epoch 16/300\n",
            "8/8 [==============================] - 1s 146ms/step - loss: 10581.7704 - reconstruction_loss: 10589.2520 - kl_loss: 0.0711\n",
            "Epoch 17/300\n",
            "8/8 [==============================] - 1s 147ms/step - loss: 10575.1580 - reconstruction_loss: 10574.6064 - kl_loss: 0.0647\n",
            "Epoch 18/300\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 10567.2697 - reconstruction_loss: 10565.9102 - kl_loss: 0.0592\n",
            "Epoch 19/300\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 10571.5068 - reconstruction_loss: 10558.3477 - kl_loss: 0.0541\n",
            "Epoch 20/300\n",
            "8/8 [==============================] - 1s 151ms/step - loss: 10567.4194 - reconstruction_loss: 10554.1641 - kl_loss: 0.0457\n",
            "Epoch 21/300\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 10532.5359 - reconstruction_loss: 10551.4541 - kl_loss: 0.0305\n",
            "Epoch 22/300\n",
            "8/8 [==============================] - 1s 147ms/step - loss: 10545.9146 - reconstruction_loss: 10543.7832 - kl_loss: 0.0162\n",
            "Epoch 23/300\n",
            "8/8 [==============================] - 1s 147ms/step - loss: 10523.3891 - reconstruction_loss: 10535.6836 - kl_loss: 0.0111\n",
            "Epoch 24/300\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 10538.6201 - reconstruction_loss: 10529.4834 - kl_loss: 0.0083\n",
            "Epoch 25/300\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 10540.4609 - reconstruction_loss: 10518.6104 - kl_loss: 0.0084\n",
            "Epoch 26/300\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 10523.4044 - reconstruction_loss: 10515.8613 - kl_loss: 0.0134\n",
            "Epoch 27/300\n",
            "8/8 [==============================] - 1s 148ms/step - loss: 10516.0266 - reconstruction_loss: 10515.7422 - kl_loss: 0.0153\n",
            "Epoch 28/300\n",
            "8/8 [==============================] - 1s 149ms/step - loss: 10509.0791 - reconstruction_loss: 10500.4785 - kl_loss: 0.0581\n",
            "Epoch 29/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 10463.2708 - reconstruction_loss: 10467.1631 - kl_loss: 0.6414\n",
            "Epoch 30/300\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 10525.1404 - reconstruction_loss: 10534.8701 - kl_loss: 8.7541\n",
            "Epoch 31/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 10488.7178 - reconstruction_loss: 10468.0215 - kl_loss: 2.1579\n",
            "Epoch 32/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 10463.9437 - reconstruction_loss: 10437.3770 - kl_loss: 6.0717\n",
            "Epoch 33/300\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 10411.9031 - reconstruction_loss: 10404.3867 - kl_loss: 7.7939\n",
            "Epoch 34/300\n",
            "8/8 [==============================] - 1s 149ms/step - loss: 10396.5725 - reconstruction_loss: 10379.6787 - kl_loss: 5.7987\n",
            "Epoch 35/300\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 10379.2089 - reconstruction_loss: 10365.1172 - kl_loss: 4.7856\n",
            "Epoch 36/300\n",
            "8/8 [==============================] - 1s 151ms/step - loss: 10332.2676 - reconstruction_loss: 10327.3496 - kl_loss: 8.0454\n",
            "Epoch 37/300\n",
            "8/8 [==============================] - 1s 151ms/step - loss: 10296.3735 - reconstruction_loss: 10255.1123 - kl_loss: 15.3014\n",
            "Epoch 38/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 10226.0943 - reconstruction_loss: 10193.6553 - kl_loss: 20.7297\n",
            "Epoch 39/300\n",
            "8/8 [==============================] - 1s 151ms/step - loss: 10165.4610 - reconstruction_loss: 10152.6514 - kl_loss: 22.8018\n",
            "Epoch 40/300\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 10137.4995 - reconstruction_loss: 10106.6758 - kl_loss: 26.2268\n",
            "Epoch 41/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 10104.1686 - reconstruction_loss: 10068.6270 - kl_loss: 28.5938\n",
            "Epoch 42/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 10070.2900 - reconstruction_loss: 10045.2617 - kl_loss: 28.1100\n",
            "Epoch 43/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 10051.5380 - reconstruction_loss: 10027.3281 - kl_loss: 29.6687\n",
            "Epoch 44/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 10027.6471 - reconstruction_loss: 10007.6709 - kl_loss: 29.8917\n",
            "Epoch 45/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 10010.1010 - reconstruction_loss: 9988.5762 - kl_loss: 29.9467\n",
            "Epoch 46/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9995.7617 - reconstruction_loss: 9966.4912 - kl_loss: 30.8410\n",
            "Epoch 47/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 9983.0499 - reconstruction_loss: 9949.6484 - kl_loss: 31.4156\n",
            "Epoch 48/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9981.6176 - reconstruction_loss: 9929.0205 - kl_loss: 32.6602\n",
            "Epoch 49/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9967.4140 - reconstruction_loss: 9905.8848 - kl_loss: 34.9827\n",
            "Epoch 50/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9934.9900 - reconstruction_loss: 9886.8252 - kl_loss: 35.7121\n",
            "Epoch 51/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 9903.8094 - reconstruction_loss: 9858.0596 - kl_loss: 35.6998\n",
            "Epoch 52/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9878.0658 - reconstruction_loss: 9850.2158 - kl_loss: 36.2983\n",
            "Epoch 53/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9880.5626 - reconstruction_loss: 9836.5107 - kl_loss: 36.6148\n",
            "Epoch 54/300\n",
            "8/8 [==============================] - 1s 158ms/step - loss: 9855.1917 - reconstruction_loss: 9824.9951 - kl_loss: 36.5584\n",
            "Epoch 55/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9849.0109 - reconstruction_loss: 9804.3438 - kl_loss: 37.7858\n",
            "Epoch 56/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9817.1360 - reconstruction_loss: 9791.8223 - kl_loss: 38.1529\n",
            "Epoch 57/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9824.3201 - reconstruction_loss: 9780.5996 - kl_loss: 39.0816\n",
            "Epoch 58/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9817.3566 - reconstruction_loss: 9776.0752 - kl_loss: 38.6796\n",
            "Epoch 59/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9787.9348 - reconstruction_loss: 9759.0918 - kl_loss: 38.8252\n",
            "Epoch 60/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9803.5143 - reconstruction_loss: 9747.0400 - kl_loss: 39.5741\n",
            "Epoch 61/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9782.2689 - reconstruction_loss: 9724.9102 - kl_loss: 39.4834\n",
            "Epoch 62/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 9779.4151 - reconstruction_loss: 9720.5762 - kl_loss: 40.1821\n",
            "Epoch 63/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9769.1772 - reconstruction_loss: 9711.8115 - kl_loss: 39.9509\n",
            "Epoch 64/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9729.6937 - reconstruction_loss: 9703.7500 - kl_loss: 39.6724\n",
            "Epoch 65/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9734.1662 - reconstruction_loss: 9690.0625 - kl_loss: 40.7215\n",
            "Epoch 66/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9701.8151 - reconstruction_loss: 9688.5156 - kl_loss: 40.8038\n",
            "Epoch 67/300\n",
            "8/8 [==============================] - 1s 151ms/step - loss: 9722.5078 - reconstruction_loss: 9678.9336 - kl_loss: 40.7632\n",
            "Epoch 68/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9714.5482 - reconstruction_loss: 9674.0000 - kl_loss: 40.0655\n",
            "Epoch 69/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9683.8916 - reconstruction_loss: 9645.2520 - kl_loss: 41.2668\n",
            "Epoch 70/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9650.5792 - reconstruction_loss: 9605.6387 - kl_loss: 41.2164\n",
            "Epoch 71/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9579.6355 - reconstruction_loss: 9553.9648 - kl_loss: 41.0641\n",
            "Epoch 72/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9577.9238 - reconstruction_loss: 9523.1377 - kl_loss: 40.5318\n",
            "Epoch 73/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9552.3962 - reconstruction_loss: 9523.5488 - kl_loss: 41.4346\n",
            "Epoch 74/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9562.0516 - reconstruction_loss: 9503.7314 - kl_loss: 41.4681\n",
            "Epoch 75/300\n",
            "8/8 [==============================] - 1s 157ms/step - loss: 9537.6549 - reconstruction_loss: 9491.1387 - kl_loss: 41.5233\n",
            "Epoch 76/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9524.8903 - reconstruction_loss: 9480.9736 - kl_loss: 41.9227\n",
            "Epoch 77/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9531.3968 - reconstruction_loss: 9488.4600 - kl_loss: 41.2349\n",
            "Epoch 78/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 9512.7197 - reconstruction_loss: 9484.5938 - kl_loss: 41.3918\n",
            "Epoch 79/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9517.8023 - reconstruction_loss: 9471.6055 - kl_loss: 41.4001\n",
            "Epoch 80/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9509.9027 - reconstruction_loss: 9469.6504 - kl_loss: 41.1776\n",
            "Epoch 81/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9514.8621 - reconstruction_loss: 9464.0918 - kl_loss: 41.5813\n",
            "Epoch 82/300\n",
            "8/8 [==============================] - 1s 151ms/step - loss: 9490.9058 - reconstruction_loss: 9462.9971 - kl_loss: 41.4271\n",
            "Epoch 83/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9496.3168 - reconstruction_loss: 9460.9824 - kl_loss: 41.4854\n",
            "Epoch 84/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9474.7450 - reconstruction_loss: 9453.4395 - kl_loss: 42.1566\n",
            "Epoch 85/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9512.2720 - reconstruction_loss: 9444.1914 - kl_loss: 42.0246\n",
            "Epoch 86/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9471.9303 - reconstruction_loss: 9444.3730 - kl_loss: 41.0720\n",
            "Epoch 87/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9470.3143 - reconstruction_loss: 9436.8320 - kl_loss: 41.5176\n",
            "Epoch 88/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9479.8135 - reconstruction_loss: 9434.1182 - kl_loss: 42.2146\n",
            "Epoch 89/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9477.1877 - reconstruction_loss: 9444.3457 - kl_loss: 41.7781\n",
            "Epoch 90/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 9486.6901 - reconstruction_loss: 9428.7617 - kl_loss: 41.6877\n",
            "Epoch 91/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9455.8356 - reconstruction_loss: 9426.9785 - kl_loss: 41.7529\n",
            "Epoch 92/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9458.4893 - reconstruction_loss: 9425.2236 - kl_loss: 41.9804\n",
            "Epoch 93/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9450.3959 - reconstruction_loss: 9422.3945 - kl_loss: 41.3253\n",
            "Epoch 94/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9437.0684 - reconstruction_loss: 9422.7295 - kl_loss: 41.7927\n",
            "Epoch 95/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9459.4022 - reconstruction_loss: 9414.5273 - kl_loss: 42.0263\n",
            "Epoch 96/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9456.6910 - reconstruction_loss: 9411.0898 - kl_loss: 41.9198\n",
            "Epoch 97/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9466.4380 - reconstruction_loss: 9412.4980 - kl_loss: 41.3378\n",
            "Epoch 98/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9463.9665 - reconstruction_loss: 9408.1055 - kl_loss: 41.2890\n",
            "Epoch 99/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9454.5837 - reconstruction_loss: 9410.2979 - kl_loss: 41.5809\n",
            "Epoch 100/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9456.4515 - reconstruction_loss: 9400.7930 - kl_loss: 42.2637\n",
            "Epoch 101/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9455.1933 - reconstruction_loss: 9401.8506 - kl_loss: 41.2868\n",
            "Epoch 102/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9413.6159 - reconstruction_loss: 9405.7197 - kl_loss: 41.2862\n",
            "Epoch 103/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9442.6009 - reconstruction_loss: 9402.6553 - kl_loss: 41.6216\n",
            "Epoch 104/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9439.1944 - reconstruction_loss: 9400.3525 - kl_loss: 41.3160\n",
            "Epoch 105/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9453.0741 - reconstruction_loss: 9391.7725 - kl_loss: 40.9188\n",
            "Epoch 106/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9452.6527 - reconstruction_loss: 9393.7256 - kl_loss: 41.5278\n",
            "Epoch 107/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9441.4219 - reconstruction_loss: 9404.0156 - kl_loss: 41.5718\n",
            "Epoch 108/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9441.3301 - reconstruction_loss: 9398.7090 - kl_loss: 40.6919\n",
            "Epoch 109/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9450.5563 - reconstruction_loss: 9383.5527 - kl_loss: 41.4514\n",
            "Epoch 110/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9439.0400 - reconstruction_loss: 9384.5205 - kl_loss: 41.1246\n",
            "Epoch 111/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9435.2789 - reconstruction_loss: 9381.5186 - kl_loss: 41.6024\n",
            "Epoch 112/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9424.0296 - reconstruction_loss: 9378.0117 - kl_loss: 41.3064\n",
            "Epoch 113/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 9417.6536 - reconstruction_loss: 9382.5010 - kl_loss: 41.1814\n",
            "Epoch 114/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9426.1249 - reconstruction_loss: 9383.3613 - kl_loss: 41.6034\n",
            "Epoch 115/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9420.2865 - reconstruction_loss: 9385.5195 - kl_loss: 40.8121\n",
            "Epoch 116/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9414.3021 - reconstruction_loss: 9387.6631 - kl_loss: 40.9287\n",
            "Epoch 117/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9422.2861 - reconstruction_loss: 9380.4766 - kl_loss: 40.8904\n",
            "Epoch 118/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9416.4652 - reconstruction_loss: 9376.8818 - kl_loss: 41.0938\n",
            "Epoch 119/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9432.5996 - reconstruction_loss: 9380.4688 - kl_loss: 40.8864\n",
            "Epoch 120/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9410.8205 - reconstruction_loss: 9373.9121 - kl_loss: 40.9229\n",
            "Epoch 121/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9407.2382 - reconstruction_loss: 9366.5742 - kl_loss: 41.0512\n",
            "Epoch 122/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9406.4402 - reconstruction_loss: 9375.7793 - kl_loss: 40.7563\n",
            "Epoch 123/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9407.6766 - reconstruction_loss: 9374.2129 - kl_loss: 40.6200\n",
            "Epoch 124/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9404.8166 - reconstruction_loss: 9375.3379 - kl_loss: 40.4183\n",
            "Epoch 125/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9407.1841 - reconstruction_loss: 9370.8408 - kl_loss: 40.4163\n",
            "Epoch 126/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9404.3813 - reconstruction_loss: 9369.8730 - kl_loss: 40.7811\n",
            "Epoch 127/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9414.9067 - reconstruction_loss: 9360.2051 - kl_loss: 40.5768\n",
            "Epoch 128/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9415.3441 - reconstruction_loss: 9363.4521 - kl_loss: 40.4394\n",
            "Epoch 129/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9406.4907 - reconstruction_loss: 9361.0713 - kl_loss: 40.4607\n",
            "Epoch 130/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9399.7652 - reconstruction_loss: 9365.5537 - kl_loss: 40.5218\n",
            "Epoch 131/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9423.7376 - reconstruction_loss: 9359.0771 - kl_loss: 40.2535\n",
            "Epoch 132/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9381.4684 - reconstruction_loss: 9360.3496 - kl_loss: 40.1767\n",
            "Epoch 133/300\n",
            "8/8 [==============================] - 1s 158ms/step - loss: 9398.9912 - reconstruction_loss: 9364.3242 - kl_loss: 40.3677\n",
            "Epoch 134/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9404.0065 - reconstruction_loss: 9357.3584 - kl_loss: 40.2534\n",
            "Epoch 135/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9391.8504 - reconstruction_loss: 9360.0234 - kl_loss: 39.6186\n",
            "Epoch 136/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9399.6237 - reconstruction_loss: 9350.4287 - kl_loss: 40.6221\n",
            "Epoch 137/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9381.6935 - reconstruction_loss: 9355.2656 - kl_loss: 39.9332\n",
            "Epoch 138/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9403.0837 - reconstruction_loss: 9362.4863 - kl_loss: 40.1812\n",
            "Epoch 139/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9393.7091 - reconstruction_loss: 9353.6436 - kl_loss: 39.9812\n",
            "Epoch 140/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9379.6519 - reconstruction_loss: 9348.2246 - kl_loss: 40.0414\n",
            "Epoch 141/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9363.1509 - reconstruction_loss: 9348.4863 - kl_loss: 40.5792\n",
            "Epoch 142/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9384.2095 - reconstruction_loss: 9344.4863 - kl_loss: 40.1231\n",
            "Epoch 143/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9389.1662 - reconstruction_loss: 9348.1123 - kl_loss: 40.0727\n",
            "Epoch 144/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9388.3347 - reconstruction_loss: 9338.0518 - kl_loss: 40.3664\n",
            "Epoch 145/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9390.7842 - reconstruction_loss: 9341.8584 - kl_loss: 39.7126\n",
            "Epoch 146/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9401.4778 - reconstruction_loss: 9339.8857 - kl_loss: 39.6556\n",
            "Epoch 147/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9379.7803 - reconstruction_loss: 9337.9053 - kl_loss: 39.7887\n",
            "Epoch 148/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9372.0000 - reconstruction_loss: 9343.7803 - kl_loss: 39.6731\n",
            "Epoch 149/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9367.5233 - reconstruction_loss: 9339.5557 - kl_loss: 39.5683\n",
            "Epoch 150/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9368.9569 - reconstruction_loss: 9338.7510 - kl_loss: 39.6739\n",
            "Epoch 151/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9384.1228 - reconstruction_loss: 9331.9902 - kl_loss: 39.7865\n",
            "Epoch 152/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9374.5787 - reconstruction_loss: 9334.8486 - kl_loss: 39.4497\n",
            "Epoch 153/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9355.1366 - reconstruction_loss: 9330.7100 - kl_loss: 39.4251\n",
            "Epoch 154/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9389.1953 - reconstruction_loss: 9334.0850 - kl_loss: 39.5536\n",
            "Epoch 155/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9382.8331 - reconstruction_loss: 9334.4902 - kl_loss: 39.4018\n",
            "Epoch 156/300\n",
            "8/8 [==============================] - 1s 157ms/step - loss: 9378.4952 - reconstruction_loss: 9330.9727 - kl_loss: 39.4568\n",
            "Epoch 157/300\n",
            "8/8 [==============================] - 1s 157ms/step - loss: 9377.2388 - reconstruction_loss: 9334.6602 - kl_loss: 39.5667\n",
            "Epoch 158/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9369.5764 - reconstruction_loss: 9330.1855 - kl_loss: 39.0835\n",
            "Epoch 159/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9366.6208 - reconstruction_loss: 9329.0547 - kl_loss: 39.0573\n",
            "Epoch 160/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9361.4919 - reconstruction_loss: 9325.3721 - kl_loss: 39.2149\n",
            "Epoch 161/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9369.4804 - reconstruction_loss: 9329.9248 - kl_loss: 38.9525\n",
            "Epoch 162/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9371.7479 - reconstruction_loss: 9325.5967 - kl_loss: 38.9209\n",
            "Epoch 163/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9353.7018 - reconstruction_loss: 9322.7568 - kl_loss: 39.4018\n",
            "Epoch 164/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9362.2930 - reconstruction_loss: 9325.7773 - kl_loss: 39.3637\n",
            "Epoch 165/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9355.3141 - reconstruction_loss: 9325.0693 - kl_loss: 38.8596\n",
            "Epoch 166/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9348.8274 - reconstruction_loss: 9326.4531 - kl_loss: 38.6649\n",
            "Epoch 167/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9374.4960 - reconstruction_loss: 9327.9619 - kl_loss: 38.8426\n",
            "Epoch 168/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9376.3215 - reconstruction_loss: 9327.2520 - kl_loss: 38.8777\n",
            "Epoch 169/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9366.3590 - reconstruction_loss: 9336.9932 - kl_loss: 38.7810\n",
            "Epoch 170/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9366.1946 - reconstruction_loss: 9327.1338 - kl_loss: 39.1678\n",
            "Epoch 171/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9351.6837 - reconstruction_loss: 9324.0508 - kl_loss: 38.7928\n",
            "Epoch 172/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9370.5265 - reconstruction_loss: 9315.8428 - kl_loss: 38.2554\n",
            "Epoch 173/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9372.0574 - reconstruction_loss: 9319.7549 - kl_loss: 38.2316\n",
            "Epoch 174/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9356.2252 - reconstruction_loss: 9319.6201 - kl_loss: 38.7787\n",
            "Epoch 175/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9377.2441 - reconstruction_loss: 9316.5625 - kl_loss: 38.7481\n",
            "Epoch 176/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9367.5445 - reconstruction_loss: 9316.9766 - kl_loss: 38.3247\n",
            "Epoch 177/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9378.4470 - reconstruction_loss: 9315.3691 - kl_loss: 38.0237\n",
            "Epoch 178/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9349.5668 - reconstruction_loss: 9315.6689 - kl_loss: 38.4749\n",
            "Epoch 179/300\n",
            "8/8 [==============================] - 1s 158ms/step - loss: 9357.6189 - reconstruction_loss: 9309.4424 - kl_loss: 38.1401\n",
            "Epoch 180/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9356.2651 - reconstruction_loss: 9316.0068 - kl_loss: 37.7376\n",
            "Epoch 181/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 9364.8319 - reconstruction_loss: 9314.1367 - kl_loss: 38.3295\n",
            "Epoch 182/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9356.8391 - reconstruction_loss: 9315.2314 - kl_loss: 38.3254\n",
            "Epoch 183/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9362.7146 - reconstruction_loss: 9311.8564 - kl_loss: 37.9394\n",
            "Epoch 184/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9345.5685 - reconstruction_loss: 9308.4697 - kl_loss: 38.2876\n",
            "Epoch 185/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9382.0689 - reconstruction_loss: 9310.3770 - kl_loss: 38.0939\n",
            "Epoch 186/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9360.1816 - reconstruction_loss: 9314.9775 - kl_loss: 37.7347\n",
            "Epoch 187/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9371.9017 - reconstruction_loss: 9309.3105 - kl_loss: 37.8320\n",
            "Epoch 188/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9352.1032 - reconstruction_loss: 9314.1133 - kl_loss: 37.9546\n",
            "Epoch 189/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9360.0247 - reconstruction_loss: 9303.3994 - kl_loss: 38.3189\n",
            "Epoch 190/300\n",
            "8/8 [==============================] - 1s 158ms/step - loss: 9346.4787 - reconstruction_loss: 9318.5400 - kl_loss: 38.3140\n",
            "Epoch 191/300\n",
            "8/8 [==============================] - 1s 157ms/step - loss: 9365.8213 - reconstruction_loss: 9313.6143 - kl_loss: 37.8621\n",
            "Epoch 192/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9346.7699 - reconstruction_loss: 9315.4941 - kl_loss: 37.7952\n",
            "Epoch 193/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9355.7190 - reconstruction_loss: 9311.5430 - kl_loss: 37.8179\n",
            "Epoch 194/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9354.9090 - reconstruction_loss: 9311.9912 - kl_loss: 37.7567\n",
            "Epoch 195/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9355.1440 - reconstruction_loss: 9313.0195 - kl_loss: 37.6793\n",
            "Epoch 196/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9367.8847 - reconstruction_loss: 9309.1025 - kl_loss: 38.1297\n",
            "Epoch 197/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9373.6497 - reconstruction_loss: 9316.0742 - kl_loss: 37.8854\n",
            "Epoch 198/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9357.5234 - reconstruction_loss: 9307.5020 - kl_loss: 37.5276\n",
            "Epoch 199/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 9349.8104 - reconstruction_loss: 9308.4062 - kl_loss: 37.4440\n",
            "Epoch 200/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9368.9628 - reconstruction_loss: 9311.3389 - kl_loss: 37.4881\n",
            "Epoch 201/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9346.8961 - reconstruction_loss: 9308.6895 - kl_loss: 37.8950\n",
            "Epoch 202/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9330.9410 - reconstruction_loss: 9317.0850 - kl_loss: 37.5100\n",
            "Epoch 203/300\n",
            "8/8 [==============================] - 1s 158ms/step - loss: 9363.3188 - reconstruction_loss: 9299.5674 - kl_loss: 36.9811\n",
            "Epoch 204/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9348.8071 - reconstruction_loss: 9305.9141 - kl_loss: 37.5745\n",
            "Epoch 205/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9360.1170 - reconstruction_loss: 9300.8281 - kl_loss: 37.9841\n",
            "Epoch 206/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9334.6705 - reconstruction_loss: 9309.7021 - kl_loss: 37.3731\n",
            "Epoch 207/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9343.3438 - reconstruction_loss: 9309.7061 - kl_loss: 37.0632\n",
            "Epoch 208/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9340.5718 - reconstruction_loss: 9303.2441 - kl_loss: 37.5976\n",
            "Epoch 209/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9350.2597 - reconstruction_loss: 9300.7217 - kl_loss: 37.4912\n",
            "Epoch 210/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9338.2299 - reconstruction_loss: 9307.3018 - kl_loss: 36.9291\n",
            "Epoch 211/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9342.2615 - reconstruction_loss: 9307.2881 - kl_loss: 37.2708\n",
            "Epoch 212/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9345.1326 - reconstruction_loss: 9305.5264 - kl_loss: 36.9528\n",
            "Epoch 213/300\n",
            "8/8 [==============================] - 1s 157ms/step - loss: 9338.5311 - reconstruction_loss: 9301.0830 - kl_loss: 36.9001\n",
            "Epoch 214/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9329.6605 - reconstruction_loss: 9306.7119 - kl_loss: 37.1089\n",
            "Epoch 215/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9335.9222 - reconstruction_loss: 9306.1387 - kl_loss: 37.2685\n",
            "Epoch 216/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9325.6323 - reconstruction_loss: 9303.3516 - kl_loss: 37.2594\n",
            "Epoch 217/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9340.9870 - reconstruction_loss: 9306.3594 - kl_loss: 37.0017\n",
            "Epoch 218/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9353.7742 - reconstruction_loss: 9312.4297 - kl_loss: 36.9158\n",
            "Epoch 219/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9344.0481 - reconstruction_loss: 9312.2012 - kl_loss: 36.8613\n",
            "Epoch 220/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 9341.5831 - reconstruction_loss: 9304.5029 - kl_loss: 36.8681\n",
            "Epoch 221/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9343.1032 - reconstruction_loss: 9298.3779 - kl_loss: 36.8919\n",
            "Epoch 222/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9341.6241 - reconstruction_loss: 9312.0312 - kl_loss: 36.9010\n",
            "Epoch 223/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9339.3905 - reconstruction_loss: 9296.0420 - kl_loss: 36.8301\n",
            "Epoch 224/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9347.6438 - reconstruction_loss: 9298.1152 - kl_loss: 36.8620\n",
            "Epoch 225/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9361.4078 - reconstruction_loss: 9295.8135 - kl_loss: 36.8670\n",
            "Epoch 226/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9337.6331 - reconstruction_loss: 9293.8447 - kl_loss: 36.7377\n",
            "Epoch 227/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9351.6520 - reconstruction_loss: 9299.6650 - kl_loss: 36.3230\n",
            "Epoch 228/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9353.0443 - reconstruction_loss: 9297.8984 - kl_loss: 36.9903\n",
            "Epoch 229/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9325.4078 - reconstruction_loss: 9299.1006 - kl_loss: 36.4404\n",
            "Epoch 230/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9345.7325 - reconstruction_loss: 9302.4209 - kl_loss: 36.2985\n",
            "Epoch 231/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 9333.1636 - reconstruction_loss: 9296.3105 - kl_loss: 36.9427\n",
            "Epoch 232/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9320.8977 - reconstruction_loss: 9295.1016 - kl_loss: 36.4057\n",
            "Epoch 233/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9326.2365 - reconstruction_loss: 9298.4668 - kl_loss: 36.2903\n",
            "Epoch 234/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9332.5001 - reconstruction_loss: 9301.6768 - kl_loss: 36.5088\n",
            "Epoch 235/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9331.0234 - reconstruction_loss: 9297.5645 - kl_loss: 36.4989\n",
            "Epoch 236/300\n",
            "8/8 [==============================] - 1s 157ms/step - loss: 9324.1729 - reconstruction_loss: 9291.7852 - kl_loss: 36.2814\n",
            "Epoch 237/300\n",
            "8/8 [==============================] - 1s 158ms/step - loss: 9347.9689 - reconstruction_loss: 9290.5420 - kl_loss: 36.3093\n",
            "Epoch 238/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9319.7791 - reconstruction_loss: 9289.3193 - kl_loss: 36.2854\n",
            "Epoch 239/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9320.8703 - reconstruction_loss: 9292.5801 - kl_loss: 36.6417\n",
            "Epoch 240/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9347.7597 - reconstruction_loss: 9292.9570 - kl_loss: 36.5735\n",
            "Epoch 241/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9332.9677 - reconstruction_loss: 9296.0293 - kl_loss: 36.2548\n",
            "Epoch 242/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9333.3491 - reconstruction_loss: 9289.2607 - kl_loss: 36.1240\n",
            "Epoch 243/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9330.8071 - reconstruction_loss: 9296.9189 - kl_loss: 36.0908\n",
            "Epoch 244/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9342.7777 - reconstruction_loss: 9288.1836 - kl_loss: 35.9182\n",
            "Epoch 245/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9346.4009 - reconstruction_loss: 9287.7012 - kl_loss: 35.9191\n",
            "Epoch 246/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9309.5895 - reconstruction_loss: 9292.7852 - kl_loss: 36.3064\n",
            "Epoch 247/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9327.5945 - reconstruction_loss: 9291.7998 - kl_loss: 36.2118\n",
            "Epoch 248/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9325.6466 - reconstruction_loss: 9290.7441 - kl_loss: 36.1186\n",
            "Epoch 249/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9320.9373 - reconstruction_loss: 9293.6250 - kl_loss: 36.0005\n",
            "Epoch 250/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9326.3786 - reconstruction_loss: 9290.3604 - kl_loss: 36.4264\n",
            "Epoch 251/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9330.2056 - reconstruction_loss: 9289.9785 - kl_loss: 35.9226\n",
            "Epoch 252/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9336.0556 - reconstruction_loss: 9287.5557 - kl_loss: 35.7534\n",
            "Epoch 253/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9318.3492 - reconstruction_loss: 9289.3145 - kl_loss: 36.2477\n",
            "Epoch 254/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9330.7980 - reconstruction_loss: 9298.7842 - kl_loss: 36.1296\n",
            "Epoch 255/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9341.4242 - reconstruction_loss: 9289.8535 - kl_loss: 35.9882\n",
            "Epoch 256/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9312.7215 - reconstruction_loss: 9290.4199 - kl_loss: 35.7433\n",
            "Epoch 257/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9333.2645 - reconstruction_loss: 9287.0732 - kl_loss: 36.1236\n",
            "Epoch 258/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9328.7104 - reconstruction_loss: 9288.9570 - kl_loss: 35.8908\n",
            "Epoch 259/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9318.3595 - reconstruction_loss: 9293.0352 - kl_loss: 35.5304\n",
            "Epoch 260/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9319.6421 - reconstruction_loss: 9286.7051 - kl_loss: 35.7723\n",
            "Epoch 261/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9347.4421 - reconstruction_loss: 9296.4385 - kl_loss: 35.8539\n",
            "Epoch 262/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9354.4612 - reconstruction_loss: 9298.2588 - kl_loss: 35.7162\n",
            "Epoch 263/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9345.5193 - reconstruction_loss: 9301.5518 - kl_loss: 35.9743\n",
            "Epoch 264/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9343.0972 - reconstruction_loss: 9284.4688 - kl_loss: 36.0140\n",
            "Epoch 265/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9331.6083 - reconstruction_loss: 9291.2344 - kl_loss: 35.6288\n",
            "Epoch 266/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9312.4071 - reconstruction_loss: 9287.4746 - kl_loss: 35.4879\n",
            "Epoch 267/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9323.5042 - reconstruction_loss: 9284.0703 - kl_loss: 35.8423\n",
            "Epoch 268/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9305.0725 - reconstruction_loss: 9287.9785 - kl_loss: 35.8699\n",
            "Epoch 269/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9318.2053 - reconstruction_loss: 9285.0391 - kl_loss: 35.5559\n",
            "Epoch 270/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9318.9013 - reconstruction_loss: 9284.5488 - kl_loss: 35.3079\n",
            "Epoch 271/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9331.2013 - reconstruction_loss: 9284.2998 - kl_loss: 35.5551\n",
            "Epoch 272/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9326.1381 - reconstruction_loss: 9282.6738 - kl_loss: 35.3953\n",
            "Epoch 273/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9321.4718 - reconstruction_loss: 9282.9229 - kl_loss: 35.6845\n",
            "Epoch 274/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9329.3231 - reconstruction_loss: 9288.4033 - kl_loss: 35.5683\n",
            "Epoch 275/300\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 9314.5010 - reconstruction_loss: 9286.3379 - kl_loss: 35.3865\n",
            "Epoch 276/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9323.0448 - reconstruction_loss: 9289.3506 - kl_loss: 35.6438\n",
            "Epoch 277/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9304.2268 - reconstruction_loss: 9283.4355 - kl_loss: 35.4795\n",
            "Epoch 278/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9330.4082 - reconstruction_loss: 9278.0195 - kl_loss: 35.5627\n",
            "Epoch 279/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9319.9452 - reconstruction_loss: 9281.8301 - kl_loss: 35.3133\n",
            "Epoch 280/300\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 9337.3995 - reconstruction_loss: 9285.4150 - kl_loss: 35.4011\n",
            "Epoch 281/300\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 9315.3212 - reconstruction_loss: 9280.1201 - kl_loss: 35.5243\n",
            "Epoch 282/300\n",
            "8/8 [==============================] - 1s 157ms/step - loss: 9300.6190 - reconstruction_loss: 9289.0000 - kl_loss: 35.2097\n",
            "Epoch 283/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9312.0311 - reconstruction_loss: 9279.8643 - kl_loss: 35.4896\n",
            "Epoch 284/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9326.5347 - reconstruction_loss: 9275.8135 - kl_loss: 35.4066\n",
            "Epoch 285/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9329.6466 - reconstruction_loss: 9284.7422 - kl_loss: 35.2597\n",
            "Epoch 286/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9314.7520 - reconstruction_loss: 9280.2773 - kl_loss: 35.4678\n",
            "Epoch 287/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9312.7717 - reconstruction_loss: 9283.6582 - kl_loss: 35.3175\n",
            "Epoch 288/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9330.2154 - reconstruction_loss: 9282.8047 - kl_loss: 35.2486\n",
            "Epoch 289/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9322.0146 - reconstruction_loss: 9270.7422 - kl_loss: 35.2069\n",
            "Epoch 290/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9309.2550 - reconstruction_loss: 9275.9873 - kl_loss: 35.3319\n",
            "Epoch 291/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9329.4291 - reconstruction_loss: 9296.0195 - kl_loss: 35.0088\n",
            "Epoch 292/300\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 9334.0530 - reconstruction_loss: 9289.7139 - kl_loss: 34.9942\n",
            "Epoch 293/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9319.4010 - reconstruction_loss: 9293.9971 - kl_loss: 35.3824\n",
            "Epoch 294/300\n",
            "8/8 [==============================] - 1s 158ms/step - loss: 9333.3411 - reconstruction_loss: 9280.0527 - kl_loss: 35.3878\n",
            "Epoch 295/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9332.3917 - reconstruction_loss: 9278.9531 - kl_loss: 34.8392\n",
            "Epoch 296/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9309.4674 - reconstruction_loss: 9277.1211 - kl_loss: 35.1090\n",
            "Epoch 297/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9318.6070 - reconstruction_loss: 9278.4990 - kl_loss: 35.4232\n",
            "Epoch 298/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9308.8448 - reconstruction_loss: 9280.0068 - kl_loss: 35.0292\n",
            "Epoch 299/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9313.0014 - reconstruction_loss: 9278.5195 - kl_loss: 34.9030\n",
            "Epoch 300/300\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 9313.0072 - reconstruction_loss: 9279.6035 - kl_loss: 35.0730\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f00920c5450>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# (x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "# mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "# images = np.expand_dims(images, -1).astype(\"float32\") / 255\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "vae.fit(images, epochs=300, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLR-I4TdbbQT",
        "outputId": "1d1e12a2-4caf-4521-9b3c-96ece802e9f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 263ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        }
      ],
      "source": [
        "digit_size = 128\n",
        "num_samples = 100\n",
        "images=[]\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    temp=((np.random.rand(latent_dim)*2)-1).tolist()\n",
        "    z_sample=np.array([temp])\n",
        "    x_decoded = (vae.decoder.predict(z_sample))*255\n",
        "    images.append(x_decoded[0].reshape(digit_size, digit_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "4HxIz6ezvn1I",
        "outputId": "cd74a395-48cd-47f7-ce97-9611b95a6b81"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=128x128 at 0x7F0028AEB310>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cv2_imshow(images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVqZs1wqu4lv"
      },
      "outputs": [],
      "source": [
        "save_dir = \"/content/gdrive/MyDrive/guided-diffusion/datasets/IITD_LQ_128/VAE_images\"\n",
        "for i in range(len(images)):\n",
        "  path = os.path.join(save_dir, str(i))\n",
        "  path += \".png\"\n",
        "  cv.imwrite(path, images[i])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
